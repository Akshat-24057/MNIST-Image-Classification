{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844376db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11c5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:14<00:00, 690kB/s] \n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 116kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 848kB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.80MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Exploratory Data Analysis...\n",
      "Class Distribution:\n",
      "Class 0: 5923 samples (9.87%)\n",
      "Class 1: 6742 samples (11.24%)\n",
      "Class 2: 5958 samples (9.93%)\n",
      "Class 3: 6131 samples (10.22%)\n",
      "Class 4: 5842 samples (9.74%)\n",
      "Class 5: 5421 samples (9.04%)\n",
      "Class 6: 5918 samples (9.86%)\n",
      "Class 7: 6265 samples (10.44%)\n",
      "Class 8: 5851 samples (9.75%)\n",
      "Class 9: 5949 samples (9.92%)\n",
      "CNNModel(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU()\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Starting model training...\n",
      "Epoch [1/15], Train Loss: 0.2160, Train Acc: 93.39%, Val Loss: 0.0449, Val Acc: 98.46%\n",
      "Epoch [2/15], Train Loss: 0.0765, Train Acc: 97.70%, Val Loss: 0.0303, Val Acc: 99.00%\n",
      "Epoch [3/15], Train Loss: 0.0571, Train Acc: 98.28%, Val Loss: 0.0305, Val Acc: 98.96%\n",
      "Epoch [4/15], Train Loss: 0.0443, Train Acc: 98.66%, Val Loss: 0.0253, Val Acc: 99.16%\n",
      "Epoch [5/15], Train Loss: 0.0391, Train Acc: 98.83%, Val Loss: 0.0270, Val Acc: 99.09%\n",
      "Epoch [6/15], Train Loss: 0.0344, Train Acc: 98.90%, Val Loss: 0.0265, Val Acc: 99.16%\n",
      "Epoch [7/15], Train Loss: 0.0323, Train Acc: 98.94%, Val Loss: 0.0242, Val Acc: 99.29%\n",
      "Epoch [8/15], Train Loss: 0.0266, Train Acc: 99.13%, Val Loss: 0.0265, Val Acc: 99.20%\n",
      "Epoch [9/15], Train Loss: 0.0227, Train Acc: 99.26%, Val Loss: 0.0246, Val Acc: 99.26%\n",
      "Epoch [10/15], Train Loss: 0.0214, Train Acc: 99.30%, Val Loss: 0.0240, Val Acc: 99.35%\n",
      "Epoch [11/15], Train Loss: 0.0185, Train Acc: 99.38%, Val Loss: 0.0252, Val Acc: 99.29%\n",
      "Epoch [12/15], Train Loss: 0.0188, Train Acc: 99.38%, Val Loss: 0.0243, Val Acc: 99.27%\n",
      "Epoch [13/15], Train Loss: 0.0164, Train Acc: 99.46%, Val Loss: 0.0313, Val Acc: 99.26%\n",
      "Epoch [14/15], Train Loss: 0.0163, Train Acc: 99.46%, Val Loss: 0.0259, Val Acc: 99.35%\n",
      "Epoch [15/15], Train Loss: 0.0148, Train Acc: 99.49%, Val Loss: 0.0244, Val Acc: 99.21%\n",
      "Evaluating the model...\n",
      "\n",
      "Model Evaluation Metrics:\n",
      "Accuracy: 0.9921\n",
      "Precision: 0.9921\n",
      "Recall: 0.9921\n",
      "F1-Score: 0.9921\n",
      "\n",
      "Most commonly misclassified digits:\n",
      "Digit 9: Misclassification rate of 0.0248\n",
      "Digit 5: Misclassification rate of 0.0101\n",
      "Digit 4: Misclassification rate of 0.0081\n",
      "\n",
      "Most common confusions:\n",
      "True digit 9 predicted as 4: 7 times\n",
      "True digit 9 predicted as 8: 7 times\n",
      "True digit 3 predicted as 5: 6 times\n",
      "True digit 4 predicted as 9: 5 times\n",
      "True digit 5 predicted as 3: 5 times\n",
      "\n",
      "Suggested improvements to the model:\n",
      "1. Increase model complexity by adding more convolutional layers\n",
      "2. Apply data augmentation techniques (rotation, shifting) to improve generalization\n",
      "3. Implement batch normalization to stabilize and accelerate training\n",
      "4. Experiment with different optimizers like SGD with momentum\n",
      "5. Use learning rate scheduling to fine-tune the training process\n",
      "\n",
      "Model saved successfully as 'mnist_cnn_model.pth'\n",
      "\n",
      "Evaluation report completed. Check the generated images for visualizations.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "num_epochs = 15\n",
    "\n",
    "# Task 1: Data Loading and Exploration\n",
    "# (a) Load the MNIST dataset using torchvision.datasets.MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])\n",
    "\n",
    "# Loading the training dataset\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Loading the test dataset\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=False, \n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Creating data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# (b) Perform exploratory data analysis (EDA)\n",
    "# Display sample images from the dataset\n",
    "def show_sample_images(dataset, num_samples=5):\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(12, 2))\n",
    "    for i in range(num_samples):\n",
    "        idx = np.random.randint(0, len(dataset))\n",
    "        img, label = dataset[idx]\n",
    "        img = img.squeeze().numpy()\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f'Label: {label}')\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sample_images.png')\n",
    "    plt.close()\n",
    "\n",
    "# Calculate and report class distribution\n",
    "def plot_class_distribution(dataset):\n",
    "    labels = [label for _, label in dataset]\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(unique_labels, counts)\n",
    "    plt.xticks(unique_labels)\n",
    "    plt.xlabel('Digit Classes')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Class Distribution in MNIST Dataset')\n",
    "    plt.savefig('class_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Class Distribution:\")\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        print(f\"Class {label}: {count} samples ({count/len(dataset)*100:.2f}%)\")\n",
    "\n",
    "# Perform EDA\n",
    "print(\"Performing Exploratory Data Analysis...\")\n",
    "show_sample_images(train_dataset)\n",
    "plot_class_distribution(train_dataset)\n",
    "\n",
    "# Task 2: CNN Implementation\n",
    "# (a) Design and implement a CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 output classes for digits 0-9\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input shape: [batch_size, 1, 28, 28]\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))  # -> [batch_size, 32, 14, 14]\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))  # -> [batch_size, 64, 7, 7]\n",
    "        x = x.view(-1, 64 * 7 * 7)                 # -> [batch_size, 64*7*7]\n",
    "        x = self.relu3(self.fc1(x))                # -> [batch_size, 128]\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)                            # -> [batch_size, 10]\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = CNNModel().to(device)\n",
    "print(model)\n",
    "\n",
    "# Task 3: Model Training\n",
    "# (a) Split data into training and testing sets (already done with train_dataset and test_dataset)\n",
    "\n",
    "# (b) Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# (c) Implement a training loop for at least 15 epochs\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs):\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_epoch_loss = val_loss / len(test_loader)\n",
    "        val_epoch_acc = 100 * val_correct / val_total\n",
    "        val_losses.append(val_epoch_loss)\n",
    "        val_accs.append(val_epoch_acc)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, '\n",
    "              f'Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.2f}%')\n",
    "    \n",
    "    # Plot training and validation accuracy/loss curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, num_epochs+1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, num_epochs+1), train_accs, label='Training Accuracy')\n",
    "    plt.plot(range(1, num_epochs+1), val_accs, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "trained_model = train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Task 4: Evaluation and Analysis\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(\"\\nModel Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Create and display confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(10), yticklabels=range(10))\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Identify most commonly misclassified digits\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    misclassification_rates = 1 - np.diag(cm_normalized)\n",
    "    most_misclassified = np.argsort(misclassification_rates)[-3:][::-1]\n",
    "    \n",
    "    print(\"\\nMost commonly misclassified digits:\")\n",
    "    for digit in most_misclassified:\n",
    "        print(f\"Digit {digit}: Misclassification rate of {misclassification_rates[digit]:.4f}\")\n",
    "    \n",
    "    # Analyze misclassifications\n",
    "    common_confusions = []\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            if i != j and cm[i, j] > 0:\n",
    "                common_confusions.append((i, j, cm[i, j]))\n",
    "    \n",
    "    common_confusions.sort(key=lambda x: x[2], reverse=True)\n",
    "    print(\"\\nMost common confusions:\")\n",
    "    for true_label, pred_label, count in common_confusions[:5]:\n",
    "        print(f\"True digit {true_label} predicted as {pred_label}: {count} times\")\n",
    "    \n",
    "    return accuracy, precision, recall, f1, cm\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating the model...\")\n",
    "accuracy, precision, recall, f1, confusion_matrix = evaluate_model(trained_model, test_loader)\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(trained_model.state_dict(), 'mnist_cnn_model.pth')\n",
    "print(\"\\nModel saved successfully as 'mnist_cnn_model.pth'\")\n",
    "\n",
    "print(\"\\nEvaluation report completed. Check the generated images for visualizations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ee6bba",
   "metadata": {},
   "source": [
    "the model can be improved by the following techniques :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33645aeb",
   "metadata": {},
   "source": [
    "The model can  be improved by the following techniques:\n",
    "\n",
    "1. Increase model complexity by adding more convolutional layers\n",
    "2. Apply data augmentation techniques (rotation, shifting) to improve generalization\n",
    "3. Implement batch normalization to stabilize and accelerate training\n",
    "4. Experiment with different optimizers like SGD with momentum\n",
    "5. Use learning rate scheduling to fine-tune the training process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
